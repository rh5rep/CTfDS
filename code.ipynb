{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h2>US 2020 elections: tweets sentiment analysis and most discussed topics</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this notebook where we will analyze the tweets for the 2020 elections using Python. The dataset can be found here: [2020 Election Tweets Dataset](https://www.kaggle.com/datasets/manchunhui/us-election-2020-tweets/data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we start by reading the csv files and creating a single panda's dataframe out of them. We then proceed to clean the data: \n",
    "- Removing links and then dropping duplicates tweets\n",
    "- Remove the tweets that do not come from one of the 51 US states (including District of Columbia)\n",
    "- Split the dataframe into 4 splits using RegEx to check if any of the two candidates are mentioned:\n",
    "    - `df` includes all the tweets from The US \n",
    "    - `filtered_tweets_trump` includes all the tweets that mention Trump but not Biden\n",
    "    - `filtered_tweets_biden` includes all the tweets that mention Biden but not Trump\n",
    "    - `filtered_tweets` includes all the tweets that mention either Trump or Biden but not both\n",
    "\n",
    "The initial dataset consist of 1747805 tweets. After the cleaning we end up with the following: `df` = 284874, `filtered_tweets_trump` = 108299, `filtered_tweets_biden` = 80177 and `filtered_tweets` = 188476. The function `process_tweet` is also defined here, and will be used later, to clean the hashtag and split them into single words (e.g. #TrumpIsOrange = Trump Is Orange). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "if os.name == \"nt\":  # For Windows\n",
    "    path_trump = path + \"\\\\data\\\\hashtag_donaldtrump.csv\"\n",
    "    path_biden = path + \"\\\\data\\\\hashtag_joebiden.csv\"\n",
    "else:  # For Unix/Linux/Mac\n",
    "    path_trump = path + \"/data/hashtag_donaldtrump.csv\"\n",
    "    path_biden = path + \"/data/hashtag_joebiden.csv\"\n",
    "\n",
    "trump = pd.read_csv(path_trump, lineterminator=\"\\n\")\n",
    "biden = pd.read_csv(path_biden, lineterminator=\"\\n\")\n",
    "trump[\"source\"] = \"Trump\"\n",
    "biden[\"source\"] = \"Biden\"\n",
    "# Concatenate and remove duplicates\n",
    "df = pd.concat([trump, biden], ignore_index=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Replace URLs with a placeholder text\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(lambda x: re.sub(r\"http\\S+\", \"[]\", x))\n",
    "# Drop duplicates based on the cleaned tweet text\n",
    "df = df.drop_duplicates(subset=[\"tweet\"])\n",
    "\n",
    "us_states = [\n",
    "    \"Alabama\",\n",
    "    \"Alaska\",\n",
    "    \"Arizona\",\n",
    "    \"Arkansas\",\n",
    "    \"California\",\n",
    "    \"Colorado\",\n",
    "    \"Connecticut\",\n",
    "    \"Delaware\",\n",
    "    \"District of Columbia\",\n",
    "    \"Florida\",\n",
    "    \"Georgia\",\n",
    "    \"Hawaii\",\n",
    "    \"Idaho\",\n",
    "    \"Illinois\",\n",
    "    \"Indiana\",\n",
    "    \"Iowa\",\n",
    "    \"Kansas\",\n",
    "    \"Kentucky\",\n",
    "    \"Louisiana\",\n",
    "    \"Maine\",\n",
    "    \"Maryland\",\n",
    "    \"Massachusetts\",\n",
    "    \"Michigan\",\n",
    "    \"Minnesota\",\n",
    "    \"Mississippi\",\n",
    "    \"Missouri\",\n",
    "    \"Montana\",\n",
    "    \"Nebraska\",\n",
    "    \"Nevada\",\n",
    "    \"New Hampshire\",\n",
    "    \"New Jersey\",\n",
    "    \"New Mexico\",\n",
    "    \"New York\",\n",
    "    \"North Carolina\",\n",
    "    \"North Dakota\",\n",
    "    \"Ohio\",\n",
    "    \"Oklahoma\",\n",
    "    \"Oregon\",\n",
    "    \"Pennsylvania\",\n",
    "    \"Rhode Island\",\n",
    "    \"South Carolina\",\n",
    "    \"South Dakota\",\n",
    "    \"Tennessee\",\n",
    "    \"Texas\",\n",
    "    \"Utah\",\n",
    "    \"Vermont\",\n",
    "    \"Virginia\",\n",
    "    \"Washington\",\n",
    "    \"West Virginia\",\n",
    "    \"Wisconsin\",\n",
    "    \"Wyoming\",\n",
    "]\n",
    "\n",
    "df = df[df[\"state\"].isin(us_states)]\n",
    "\n",
    "frequent_names_trump = [\n",
    "    \"Trump\",\n",
    "    \"Donald\" \"Donald Trump\",\n",
    "    \"@realDonaldTrump\",\n",
    "    \"The Donald\",\n",
    "    \"45\",\n",
    "    \"Donald J. Trump\",\n",
    "    \"DJT\",\n",
    "    \"The Trump Administration\",\n",
    "    \"Trumpster\",\n",
    "    \"POTUS\",\n",
    "    \"@POTUS\",\n",
    "    \"Republican\",\n",
    "    \"Republicans\",\n",
    "    \"GOP\",\n",
    "    \"MAGA\",\n",
    "    \"Right Wing\",\n",
    "    \"the Right\",\n",
    "]\n",
    "frequent_names_biden = [\n",
    "    \"Biden\",\n",
    "    \"Joe Biden\",\n",
    "    \"@JoeBiden\",\n",
    "    \"The Biden\",\n",
    "    \"46\",\n",
    "    \"Joseph R. Biden\",\n",
    "    \"JRB\",\n",
    "    \"The Biden Administration\",\n",
    "    \"Bidenster\",\n",
    "    \"Joe\",\n",
    "    \"Joseph\",\n",
    "    \"Joseph Biden\",\n",
    "    \"Sleepy Joe\",\n",
    "    \"Uncle Joe\",\n",
    "    \"Dems\",\n",
    "    \"Democrat\",\n",
    "    \"Democrats\",\n",
    "    \"Left Wing\",\n",
    "    \"The Left\",\n",
    "]\n",
    "pattern_trump = \"|\".join(frequent_names_trump)\n",
    "pattern_biden = \"|\".join(frequent_names_biden)\n",
    "\n",
    "# Create boolean masks where tweets contain any of the frequent names\n",
    "mask_trump = df[\"tweet\"].str.contains(pattern_trump, case=False, na=False)\n",
    "mask_biden = df[\"tweet\"].str.contains(pattern_biden, case=False, na=False)\n",
    "\n",
    "# Combine the masks to filter for tweets containing Trump names but not Biden names or vice versa\n",
    "testdf = df.copy()\n",
    "testdf[\"tweet_about\"] = \"None\"\n",
    "filtered_tweets_trump = df[mask_trump & ~mask_biden]\n",
    "filtered_tweets_biden = df[mask_biden & ~mask_trump]\n",
    "\n",
    "testdf.loc[mask_trump & ~mask_biden, \"tweet_about\"] = \"trump\"\n",
    "testdf.loc[mask_biden & ~mask_trump, \"tweet_about\"] = \"biden\"\n",
    "\n",
    "filtered_tweets = testdf[(mask_trump & ~mask_biden) ^ (mask_biden & ~mask_trump)]\n",
    "filtered_tweets = filtered_tweets.drop_duplicates(subset=[\"tweet\"])\n",
    "\n",
    "\n",
    "def process_tweet(text):\n",
    "    def split_hashtag(match):\n",
    "        words = wordninja.split(match.group())\n",
    "        return \" \".join(words)\n",
    "\n",
    "    # Remove all URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Replace all hashtags with split words\n",
    "    processed_text = re.sub(r\"#\\w+\\b\", split_hashtag, text)\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# text = \"Check this out: https://example.com The US is the Best Country in the world! #USA #US #America!! #UnitedStates!\"\n",
    "# processed_text = process_tweet(text)\n",
    "# print(processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
